## Самигуллин Равиль гр6133
Я сделал лабараторную на googlecollab так как у меня нет, возможности реализовать GPU на своем potatoPC.

Для реализации на c++ я использовал инструкцию на [geekforgeeks](https://www.geeksforgeeks.org/how-to-run-cuda-c-c-on-jupyter-notebook-in-google-colaboratory/).

Я определил индексы строки (row) и столбца (col) для каждой нити:

`int row = blockIdx.y * blockDim.y + threadIdx.y;`

`int col = blockIdx.x * blockDim.x + threadIdx.x;`

Используя условие `if (row < rowsA && col < colsB)` каждая нить проверяет что её индексы находятся в пределах размеров результатирующей матрицы

Каждая нить вычисляет сумму произведений элементов матриц A и B

```
for (int k = 0; k < colsA; ++k) {
    sum += A[row * colsA + k] * B[k * colsB + col];
}
```

Размеры сетки вычисляется динамически исходя из размеров матриц A, B и C, размер блока выбран как 16 x 16, не уверен, что правильно взял блок, в форумах говорили только про ограничения по видеокарте:

```
dim3 threadsPerBlock(16, 16);
dim3 blocksPerGrid((colsB + threadsPerBlock.x - 1) / threadsPerBlock.x, (rowsA + threadsPerBlock.y - 1) / threadsPerBlock.y);
```

Я разделил размеры матрицы C на размеры блока и округлил до целого.

Таблица результатов умножения время миллисекунды.

| матрица      | 10x10x10 |1000x1000x1000|100x10x1000 |1000x1000x10000|
| ---          |     ---  |  ---         |        --- | ---           |
| Правильность | CORRECT  |	CORRECT      |	CORRECT   |	CORRECT       |
| CPU Time     | 0.004 m  |	3936.7 m     |	3.029 m   |	62781.3 m     |
| GPU Time     | 0.02048 m|	6.99731 m    |	0.033184 m|	70.0657 m     |

m – milliseconds

Выводы: Меньшее количество вычислений происходит быстрее на CPU, но при увеличении количества умножений они происходят быстрее на GPU.

Параллельное выполнение операции перемножения матриц на GPU с использованием CUDA позволяет использовать мощности параллельной обработки GPU для ускорения вычислений в сравнении с последовательным выполнением на CPU.


